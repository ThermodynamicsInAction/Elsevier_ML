#import libraries
'''Step 1: import crucial libraries as numpy, pandas, sklearn preprocessing, model like XGBoost
selection functions etc.'''
#I.E.
/.../
import xgboost as xgb

'''Step 2: Prepare data set and preprocessing:
Splitting Data into Training and Testing Sets, Scaling data etc. 
The steps are analogous to what was done with the neural network
'''
df = pd.read_csv('dataset.csv',encoding = 'utf8',sep=';') 
X = df[['M_C', 'M_A', 'SYM', 'P', 'T']].values
y = df[['EXP U']].values
/.../

###Example of use XGBoost Parameters 
params = {
    'objective': 'reg:squarederror',  # Task: regression
    'max_depth': 4,                  # Max. of tree depth
    'learning_rate': 0.3,
    'random_state': 42        
}
'''Step 4: Create XGBoost model '''
xgb_model = xgb.train(params, dtrain, num_boost_round=300) 
'''Step 5: Evaluate the model, create test/train predictions, create data frames,.'''
'''Step 6: Perform 25 folds cross-validation'''
'''Step 7: Test the trained model on the given parameters of 
molar masses of cation, anion, temperatures, and pressures.
The structure is the same as that of a neural network'''
#I.E.
### A function that counts predictions for a specific ionic liquid
def predictions3(MC,MA,SYM,P,T):
/.../